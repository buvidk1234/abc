"""
æ··åˆæ£€ç´¢ vector + keyword -> Top 50
å¤šè·¯å¬å›
é‡æ’åº reranking -> Top 5
æŸ¥è¯¢é‡å†™ (LLM)
"""

import os
from typing import List, Any, TypedDict

from langchain_classic.retrievers import ContextualCompressionRetriever
from langchain_core.documents import Document
from langchain_core.runnables import RunnableConfig

# --- å…³é”®ä¾èµ– ---
from qdrant_client import QdrantClient
from langchain_qdrant import QdrantVectorStore, FastEmbedSparse
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
from langchain_cohere import CohereRerank


# ==========================================
# 1. ğŸ† æœ€ä½³é…ç½®å‚æ•° (SOTA Configuration)
# ==========================================

# Qdrant é…ç½®
QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY", None)
COLLECTION_NAME = "enterprise_knowledge_hybrid"  # å»ºè®®ç”¨æ–°åå­—ï¼Œå› ä¸ºæ•°æ®ç»“æ„å˜äº†

# Rerank é…ç½® (ä½¿ç”¨æœ€æ–°çš„ v3.5)
COHERE_API_KEY = os.getenv("COHERE_API_KEY", "your-key")
RERANK_MODEL = "rerank-multilingual-v3.5"  # âœ… å‡çº§åˆ° v3.5

# æ£€ç´¢å‚æ•° (æ¼æ–—è®¾è®¡)
TOP_K_RECALL = 50  # æ··åˆå¬å›æ•°é‡
TOP_K_RERANK = 8  # æœ€ç»ˆç»™ LLM çš„æ•°é‡ (v3.5 æ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Œå¯ä»¥ç»™å¤šç‚¹)


# ==========================================
# 2. ğŸ› ï¸ åˆå§‹åŒ– SOTA æ£€ç´¢å™¨
# ==========================================

def get_retriever():
    """
    æ„å»º [BGE-M3 æ··åˆå¬å›] + [Cohere v3.5 é‡æ’åº] çš„ç®¡é“
    """

    # 1. åˆå§‹åŒ– Qdrant å®¢æˆ·ç«¯
    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)

    # 2. å®šä¹‰ Embedding æ¨¡å‹ (å…³é”®æ­¥éª¤)
    # ä½¿ç”¨ FastEmbed è¿è¡Œ BGE-M3ï¼Œé€Ÿåº¦æå¿«ï¼Œæ— éœ€ GPU
    # ä½œç”¨: ç”Ÿæˆè¯­ä¹‰å‘é‡ (Dense)
    dense_embeddings = FastEmbedEmbeddings(
        model_name="BAAI/bge-m3"
    )

    # 3. å®šä¹‰ Sparse Embedding æ¨¡å‹ (å…³é”®æ­¥éª¤)
    # ä½œç”¨: ç”Ÿæˆå…³é”®è¯å‘é‡ (Sparse/SPLADE)ï¼Œæ›¿ä»£ä¼ ç»Ÿçš„ BM25
    # ä½¿ç”¨ Qdrant æ¨èçš„ BM42 (åŸºäº BGE ä¼˜åŒ–çš„ç¨€ç–æ¨¡å‹)
    sparse_embeddings = FastEmbedSparse(
        model_name="Qdrant/bm42-all-minilm-l6-v2-attentions"
    )

    # 4. åˆå§‹åŒ–å‘é‡åº“ (å¼€å¯æ··åˆæ£€ç´¢æ¨¡å¼)
    vector_store = QdrantVectorStore(
        client=client,
        collection_name=COLLECTION_NAME,
        embedding=dense_embeddings,
        # âœ… å¼€å¯æ··åˆæ£€ç´¢é­”æ³•
        sparse_embedding=sparse_embeddings,
        retrieval_mode="hybrid",
    )

    # 5. å®šä¹‰åŸºç¡€æ£€ç´¢å™¨ (Hybrid Retriever)
    # è¿™ä¸€æ­¥ä¼šè‡ªåŠ¨å¹¶å‘æ‰§è¡Œ: è¯­ä¹‰æœç´¢ + å…³é”®è¯æœç´¢ï¼Œå¹¶è‡ªåŠ¨èåˆåˆ†æ•°
    base_retriever = vector_store.as_retriever(
        search_type="similarity",
        search_kwargs={"k": TOP_K_RECALL}
    )

    # 6. å®šä¹‰é‡æ’åºå™¨ (Cohere v3.5)
    compressor = CohereRerank(
        cohere_api_key=COHERE_API_KEY,
        model=RERANK_MODEL,
        top_n=TOP_K_RERANK
    )

    # 7. ç»„è£…ç®¡é“
    final_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=base_retriever
    )

    return final_retriever


# å…¨å±€å•ä¾‹
_GLOBAL_RETRIEVER = get_retriever()


# ==========================================
# 3. ğŸ§© å›¾èŠ‚ç‚¹é€»è¾‘
# ==========================================

class GraphState(TypedDict):
    messages: List[Any]
    retrieved_context: str
    source_documents: List[Document]


def retrieve_node(state: GraphState, config: RunnableConfig):
    """
    ä¼ä¸šçº§æ··åˆæ£€ç´¢èŠ‚ç‚¹
    """
    print(f"--- ğŸš€ å¼€å§‹æ£€ç´¢ (Model: BGE-M3 + Cohere v3.5) ---")

    query = state["messages"][-1].content

    try:
        # è¿™ä¸€è¡Œä»£ç èƒŒåå‘ç”Ÿäº†ï¼š
        # 1. Query -> BGE-M3 -> [Dense Vector]
        # 2. Query -> BM42   -> [Sparse Vector]
        # 3. Qdrant -> Dense Search + Sparse Search -> Score Fusion -> Top 50
        # 4. Cohere -> Rerank -> Top 8
        docs = _GLOBAL_RETRIEVER.invoke(query)

        # æ ¼å¼åŒ–è¾“å‡º
        context_parts = []
        for i, doc in enumerate(docs):
            # è·å–å…ƒæ•°æ® (å‡è®¾å…¥åº“æ—¶å­˜äº† source å’Œ page)
            meta = doc.metadata
            source_info = f"{meta.get('source', 'unknown')} (P.{meta.get('page', '?')})"

            # æ‹¼æ¥: [1] å†…å®¹ (æ¥æº)
            context_parts.append(f"[{i + 1}] {doc.page_content}\n   Source: {source_info}")

        context_str = "\n\n".join(context_parts)

        print(f"âœ… æ£€ç´¢æˆåŠŸ: æœ€ç»ˆä¿ç•™ {len(docs)} æ¡é«˜ç›¸å…³æ–‡æ¡£")

        return {
            "retrieved_context": context_str,
            "source_documents": docs
        }

    except Exception as e:
        print(f"âŒ æ£€ç´¢ä¸¥é‡é”™è¯¯: {e}")
        # ç”Ÿäº§ç¯å¢ƒå»ºè®®æ¥å…¥ Sentry æˆ– Log ç›‘æ§
        return {
            "retrieved_context": "",
            "source_documents": []
        }
